---
title: 机器学习笔记
tags: 数学，计算机
categories: 学习笔记
date: 2019-05-04 17:42:17
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://wujilingfeng.top/MathJax/MathJax.js?config=TeX-AMS_CHTML">
</script>


本文记录自己对机器学习数学研究的学习笔记

<!--more-->

#### 机器学习几宗罪
* 庞大的名词学习成本，这些名词定义多简单，且冗余
* 越来越庞大的框架学习成本，这些框架多相似，且数学等价
* 重工程轻理论，学说庞杂，成本巨大，术语定义不明且繁多

#### 随机梯度下降法
对每组数据计算损失函数，然后应用梯度下降法，随机体现在随机抽取一组数据。
这种狗屁方法怎么看都不能叫做优化方法，真不知道pytorch是怎么把SGD(随机梯度下降法)放到torch.optim,机器学习为什么要增加这个名词的学习成本(我查了两天资料，佩服学计算机的人！)。

##### 信息熵

获取这里用”编码量“更”严谨“，假设一个事件的概率是$p_i$,则它的编码量是$-logp_i$,log以2为底。那么对于概率测度$\left(\Omega,A,\mu\right)$,其平均一次编码量为$H\left(p\right)=-E\left(log\mu\left(A\right)\right)$

###### 命题

$-E\left(log\mu\left(A\right)\right)=\sum p_ilog\frac{1}{p_i}\leq log\sum 1$等号成立的条件是$p_i$全相等

##### 相对熵

$KL\left(P,Q\right)=\sum p_ilog\frac{p_i}{q_i}$

易知$P=Q$时取最小值（证明方法同上）

有人用KL散度（编码量差）度量概率空间，这是不合理的。事实上概率空间可以用全变差来度量，和wassertein距离度量。准确地来说交叉熵是凸能量而不是距离，下面的凸能量存在定理也讨论了这样的事情。

如果非要用到信息熵。我们可以对概率测度P做这样的变换$\frac{1}{H\left(p\right)}p_ilogp_i  $这是新的概率测度，我们可以衡量新生成的概率测度。

在向往的生活3中陈伟霆玩的游戏中我们了解到监督学习(博弈类学习)的困难。

#### 神经元（工程）

神经元适合模拟事物规律，所以神经元适合"学习"分类，识别问题。

神经元框架（编程）考虑物理原型（人的神经元细胞）是有好处的，这样当我们制造物理模型是可以兼容。

神经元的一个模块（函数）的输入理论上都可以化为向量形式，而当我们输入一串以时间作为标志的数据量时，所以我们可以允许输入变量是二维张量，他的默认计算方式也是显然的。

人类最根本的知识（技能）来自实践(相当于反编译器)

人类从长辈（书）学习的知识（技能）相当于编译器

神经元最重要的非线性控制系统(非线性迭代)没有添加
神经元的宽度决定学习能力，深度决定能力的稳定性
~~记忆，处理，输入，输出都可以看成一类，都可以用循环表示~~

神经网络用来描述'对象'，循环神经网络存在推导(推理成分)，即归纳总结能力[^2]

[^2]: 这句话和上面的“深度决定能力的稳定性“相对应，因为循环神经网络就是增加了深度

#### 定义

逼近这里采用无穷泛数的定义
#### 命题 1

对于m个输入的函数f，(f是连续的，且$x_i$是紧集)则$\left(x_1, x_2,...x_n,f()\right)$是m维流形。那么对于任何的m维流形，一层神经网络就可以无限逼近这个流形（Relu函数）

*证明 :* 由[这里](https://wujilingfeng.top/2019/07/03/泛函分析笔记/fun_of_ana.pdf)的第四条,(欧式空间紧集上的函数以无穷泛数为泛数，他是紧的而且等价等度连续和一致有界)，我们可以在定义域$\Omega$空间找到$\epsilon$网格(在这些网格中函数差小于$\epsilon$）
然后我们可以用足够多的超平面划分这些网格。
接下来我们的手法如下：
{% asset_img 1.jpg picture1 %}
{% asset_img 2.jpg picture2 %}
{% asset_img 3.jpg picture3%}



#### 命题1（概率论）
对于m维空间$\Omega$上borel可测集，上有测度$f$。那么Relu函数也是上面的测度，且测度空间以无穷泛数作为泛数，Relu函数的线性组合可以逼近f.(用概率论的语言叙述命题1)

#### 命题2
有n层神经网络，每层(i层到i+1层)满足如下条件:
* $x \to Relu\left(W_i x+b_i\right)$,$W_i$是$m\times m$矩阵，且满秩
那么这个神经网络把凸集映为凸集


*证明：* 只需证明每层神经网络把凸集映为凸集
若$\Omega$属于支集则结论成立，若$\Omega$有不属于支集的部分，我们可以把非支集部分映到支集，保持值不变。
比如把$P_0$不属于支集，它的第一位数字超过支集，我们可以沿着$W_i$第2到第m的向量的公共法向量移动，让第一位数移到$W_i$第一个向量定义的超平面上，以下位数同理。

#### 推论
上述证明手法把定义域与支集相交，$Relu\left(W_ix+b\right)=W_ix+b$,又因为$W_i$满秩，所以可逆，故把非凸集映为非凸集
#### 推论
上面结论把Relu函数换成单调函数f也成立（比如softmax）。
由于仿射变环维持凸凹性，所以直接考察$y=f(x)$,$y_1,y_2\in f(x)$,那么$\frac{y_1+y_2}{2}\in f\left(x\right)$只需对每位数单独考虑即可

#### 推论
如上所述，若矩阵$W_i$满足行满秩，则把凸集映为凸集结论成立
* 证明: * 如上

举例说明上述条件不可弱化
若行不满秩，假设 m=3(输入向量维数)
令
$$W_i=\begin{matrix}
1&0&0\\\\
0&1&0\\\\
1&1&0
\end{matrix}$$
则$Relu\left(W_ix\right)$是非凸集
若假设m=2,则
$$W_i=\begin{matrix}
1&0\\\\
0&1\\\\
1&1
\end{matrix}$$
则$Relu\left(W_ix\right)$是非凸集

* 所以上述 结论讨论神经元微观作用，RELU函数等其他激活函数相当于对定义域进行剪枝，尤其当仿射变换矩阵是满秩的时候，这时候的神经网络功能非常弱，最起码神经网络的宽度要放大2倍。
* 而从宏观上看，输出的流形维数是小于等于输入空间的，尤其是人脸空间是高维空间的子流形，一般它的输出空间的维数远小于输入

#### 命题(称不上命题，可以叫做理解)：流形的参数化，曲面贴图(工程)，换脸神经网络的统一性
这里参数化应是同胚（双射）

网上有一些库完成换脸操作。
人脸贴图把人脸同胚映射（参数化)到圆盘，贴图也映射（参数化）到圆盘，则圆盘上对应的点便是人脸到贴图的映射。
换脸操作也是如此，人A的脸表情空间是n维流形，把它映射到单位空间，人B的脸也是如此，然后在单位空间对应点的映射便是人脸A到B的映射

#### 命题
对于任何有限元素，它们有欧式空间的参数化，则这个这些有限点集可以属于任意维（大于1）流形
例子:比如任何维度背景空间下的有限点集，可以用折线链接这些点，则这些点属于这个折线（一维流形）的元素。
这个命题用来为这样一例子提供理论解释:
对一个人有限个表情，你可以把它嵌入任意维（大于1）流形，也就是神经网络中把人脸可以映射“
编码映射“到任意维度。

#### 命题

多层神经网络即使有激活函数的作用，当行不满秩时，等价一层神经网络

*proof*待证

如此一来我们完成了神经网络的宏观理论。接下来讨论参数化后的度量(计算机中管它叫损失函数,这是不严格的，因为上面也提到他们用的KL并不是距离，不满足对称性)，上面也提到衡量测度之间的距离可以用最优传输理论，而参数化后的空间（神经网络的输出）我们可以赋以合适的测度，那么目标空间（期望输出）赋以合适的测度，空间元素之间定义合适的代价函数（不妨是欧式距离），测度空间的距离就是最优传输距离

#### 一个问题

* 若定义域和值域一致，且映射f连续（记为条件1）

则$x\to f\left(x\right)$存在不动点P，满足在P的邻域$x_i=f\left(x_{i-1}\right)$是收敛到P的？

很遗憾，猜想不成立，比如旋转的圆盘(可是除了这个刚性变换的例子，还有其他例子么？)
#### brouwer不动点定理的证明
[这里给出几个链接](https://blog.csdn.net/foxeatapple/article/details/6152992)

#### elman神经网络

#### Hopfield神经网络
#### 贝叶斯公式（定义）
与其说是公式倒不如说是定义

#### 命题 （凸能量存在定理）
若n维向量函数f$: x\in R^n \to f\left(x\right)\in R^n$的雅克比矩阵处处正定，给任意一点$p\in R^n$,则可以构造一个凸能量函数，使得此能量函数的极值点是p
证明: 由poincare lemma 知，$f_1\left(x\right)dx_1 +...+f_n\left(x\right)dx_n$是闭的微分形式，则它在欧式空间一定是恰当的，即存在F,$st. dF=f_1\left(x\right)dx_1 +...+f_n\left(x\right)dx_n$
凸能量为$F-<p,x>$


梯度消失和梯度爆炸只需要把梯度向量单位化就好阿


#### 定义   （迭代序列）

有向量函数f，初始向量$x_0$,则$\left(x_0,x_1=f\left(x_0\right),...x_i=\left(x_{x-i}\right),....\right)$为迭代序列

由上面的brower定理知，对于紧算子，其定义域与包含值域，那么存在一点p使得迭代序列收敛。
#### 命题
若复合函数$f\left(g\left(x\right)\right)$,g是紧算子，且定义域包含值域，那么也存在一点p，使得迭代序列收敛

###### 举例

$f\left(x\right)=1-\frac{x^2}{5}+x$,当$x_0\in [0,1]$迭代序列收敛，当$x_0\in [100,+\infty]$迭代序列发散

#### 神经网络添加记忆

可以使用如下手法以实现网络的记忆功能而又不改变原有神经网络框架，这样即实现框架的统一性，又减少了学习成本

对原有数据张量扩张另一维度相等的输入，然后循环。

神经网络在欧式紧集可以逼近任何函数，但是对于非紧集，却不成立，这样的例子随处可见($x^2$),而循环神经网络则可以轻易逼近

如以下函数:$y=x mod 1 \left(x>0\right)$

它的函数图像如图

无论多少层神经元都无法逼近它，而对于循环神经网络就一层便足够了。

使用循环神经网络进行网络的拓深(相当数量级的拓深)，

深度学习库推介使用pytorch

* 生物神经元中只有输入输出，没有反向传播，或者反向传播也可以看作输入输出

[^1]:  给出连续映射把闭集映为开集的例子 $\left(0,1\right)\bigcup \[2,3)\to \left(0,1\right)$的连续映射，$\left(0,1\right)$是闭集（没学过拓扑的人估计都不敢相信这句话），它的像是开集。