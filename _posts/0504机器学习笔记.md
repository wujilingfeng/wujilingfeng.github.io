---
title: 机器学习笔记
tags: 数学，计算机
categories: 学习笔记
date: 2019-05-04 17:42:17
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://wujilingfeng.top/MathJax/MathJax.js?config=TeX-AMS_CHTML">
</script>


本文记录自己对机器学习数学研究的学习笔记

<!--more-->

#### 机器学习几宗罪
* 庞大的名词学习成本，这些名词定义多简单，且冗余
* 越来越庞大的框架学习成本，这些框架多相似，且数学等价
* 重工程轻理论，学说庞杂，成本巨大，术语定义不明且繁多

#### 随机梯度下降法
对每组数据计算损失函数，然后应用梯度下降法，随机体现在随机抽取一组数据。
这种狗屁方法怎么看都不能叫做优化方法，真不知道pytorch是怎么把SGD(随机梯度下降法)放到torch.optim,机器学习为什么要增加这个名词的学习成本(我查了两天资料，佩服学计算机的人！)。

##### 信息熵

获取这里用”编码量“更”严谨“，假设一个事件的概率是$p_i$,则它的编码量是$-logp_i$,log以2为底。那么对于概率测度$\left(\Omega,A,\mu\right)$,其平均一次编码量为$H\left(p\right)=-E\left(log\mu\left(A\right)\right)$

###### 命题

$-E\left(log\mu\left(A\right)\right)=\sum p_ilog\frac{1}{p_i}\leq log\sum 1$等号成立的条件是$p_i$全相等

##### 相对熵

$KL\left(P,Q\right)=\sum p_ilog\frac{p_i}{q_i}$

易知$P=Q$时取最小值（证明方法同上）

有人用KL散度（编码量差）度量概率空间，这是不合理的。事实上概率空间可以用全变差来度量，和wassertein距离度量。

如果非要用到信息熵。我们可以对概率测度P做这样的变换$\frac{1}{H\left(p\right)}p_ilogp_i  $这是新的概率测度，我们可以衡量新生成的概率测度。

在向往的生活3中陈伟霆玩的游戏中我们了解到监督学习(博弈类学习)的困难。

#### 神经元（工程）

神经元适合模拟事物规律，所以神经元适合"学习"分类，识别问题。

神经元框架（编程）考虑物理原型（人的神经元细胞）是有好处的，这样当我们制造物理模型是可以兼容。

神经元的一个模块（函数）的输入理论上都可以化为向量形式，而当我们输入一串以时间作为标志的数据量时，所以我们可以允许输入变量是二维张量，他的默认计算方式也是显然的。

人类最根本的知识（技能）来自实践(相当于反编译器)

人类从长辈（书）学习的知识（技能）相当于编译器

神经元最重要的非线性控制系统(非线性迭代)没有添加
神经元的宽度决定学习能力，深度决定能力的稳定性
记忆，处理，输入，输出都可以看成一类，都可以用循环表示

#### 定义
逼近这里采用无穷泛数的定义
#### 命题 1

对于m个输入的函数f，(f是连续的，且$x_i$是紧集)则$\left(x_1, x_2,...x_n,f()\right)$是m维流形。那么对于任何的m维流形，一层神经网络就可以无限逼近这个流形（Relu函数）

*证明 :* 由[这里](https://wujilingfeng.top/2019/07/03/泛函分析笔记/fun_of_ana.pdf)的第四条,(欧式空间紧集上的函数以无穷泛数为泛数，他是紧的而且等价等度连续和一致有界)，我们可以在定义域$\Omega$空间找到$\epsilon$网格(在这些网格中函数差小于$\epsilon$）
然后我们可以用足够多的超平面划分这些网格。
接下来我们的手法如下：
{% asset_img 1.jpg picture1 %}
{% asset_img 2.jpg picture2 %}
{% asset_img 3.jpg picture3%}



#### 命题1（概率论）
对于m维空间$\Omega$上borel可测集，上有测度$f$。那么Relu函数也是上面的测度，且测度空间以无穷泛数作为泛数，Relu函数的线性组合可以逼近f.(用概率论的语言叙述命题1)

#### 命题2
有n层神经网络，每层(i层到i+1层)满足如下条件:
* $x \to Relu\left(W_i x+b_i\right)$,$W_i$是$m\times m$矩阵，且满秩
那么这个神经网络把凸集映为凸集


*证明：* 只需证明每层神经网络把凸集映为凸集
若$\Omega$属于支集则结论成立，若$\Omega$有不属于支集的部分，我们可以把非支集部分映到支集，保持值不变。
比如把$P_0$不属于支集，它的第一位数字超过支集，我们可以沿着$W_i$第2到第m的向量的公共法向量移动，让第一位数移到$W_i$第一个向量定义的超平面上，以下位数同理。

#### 推论
上述证明手法把定义域与支集相交，$Relu\left(W_ix+b\right)=W_ix+b$,又因为$W_i$满秩，所以可逆，故把非凸集映为非凸集
#### 推论
上面结论把Relu函数换成单调函数f也成立（比如softmax）。
由于仿射变环维持凸凹性，所以直接考察$y=f(x)$,$y_1,y_2\in f(x)$,那么$\frac{y_1+y_2}{2}\in f\left(x\right)$只需对每位数单独考虑即可

#### 推论
如上所述，若矩阵$W_i$满足行满秩，则把凸集映为凸集结论成立
* 证明: * 如上

举例说明上述条件不可弱化
若行不满秩，假设 m=3(输入向量维数)
令
$$W_i=\begin{matrix}
1&0&0\\\\
0&1&0\\\\
1&1&0
\end{matrix}
$$
则$Relu\left(W_ix\right)$是非凸集
若假设m=2,则
$$W_i=\begin{matrix}
1&0\\\\
0&1\\\\
1&1
\end{matrix}
$$
则$Relu\left(W_ix\right)$是非凸集

* 所以上述 结论讨论神经元微观作用，RELU函数等其他激活函数相当于对定义域进行剪枝，尤其当仿射变换矩阵是满秩的时候，这时候的神经网络功能非常弱，最起码神经网络的宽度要放大2倍。
* 而从宏观上看，输出的流形维数是小于等于输入空间的，尤其是人脸空间是高维空间的子流形，一般它的输出空间的维数远小于输入

#### 命题(称不上命题，可以叫做理解)：流形的参数化，曲面贴图(工程)，换脸神经网络的统一性
网上有一些库完成换脸操作。
人脸贴图把人脸同胚映射（参数化)到圆盘，贴图也映射（参数化）到圆盘，则圆盘上对应的点便是人脸到贴图的映射。
换脸操作也是如此，人A的脸表情空间是n维流形，把它映射到单位空间，人B的脸也是如此，然后在单位空间对应点的映射便是人脸A到B的映射
#### 命题
对于任何有限元素，它们有欧式空间的参数化，则这个这些有限点集可以属于任意维（大于1）流形
例子:比如任何维度背景空间下的有限点集，可以用折线链接这些点，则这些点属于这个折线（一维流形）的元素。
这个命题用来为这样一例子提供理论解释:
对一个人有限个表情，你可以把它嵌入任意维（大于1）流形，也就是神经网络中把人脸可以映射“
编码映射“到任意维度。



如此一来我们完成了神经网络的宏观理论。接下来讨论参数化后的度量(计算机中管它叫损失函数,这是不严格的，因为上面也提到他们用的KL并不是距离，不满足对称性)，上面也提到衡量测度之间的距离可以用最优传输理论，而参数化后的空间（神经网络的输出）我们可以赋以合适的测度，那么目标空间（期望输出）赋以合适的测度，空间元素之间定义合适的代价函数（不妨是欧式距离），测度空间的距离就是最优传输距离

#### 一个公开的猜想（没来得及查资料）

* 若定义域和值域一致，且映射f连续（记为条件1）

则$x\to f\left(x\right)$存在不动点P，满足在P的邻域$x_i=f\left(x_{i-1}\right)$是收敛到P的

很遗憾，猜想不成立，比如旋转的圆盘(可是除了这个刚性变换的例子，还有其他例子么？)
#### brouwer不动点定理的证明
[这里给出几个链接](https://blog.csdn.net/foxeatapple/article/details/6152992)

#### elman神经网络

#### Hopfield神经网络



梯度消失和梯度爆炸只需要把梯度向量单位化就好阿


#### 定义   （迭代序列）

有向量函数f，初始向量$x_0$,则$\left(x_0,x_1=f\left(x_0\right),...x_i=\left(x_{x-i}\right),....\right)$为迭代序列

###### 举例

$f\left(x\right)=1-\frac{x^2}{5}+x$,当$x_0\in [0,1]$迭代序列收敛，当$x_0\in [100,+\infty]$迭代序列发散

深度学习库推介使用pytorch

[^1]:  给出连续映射把闭集映为开集的例子 $\left(0,1\right)\bigcup \[2,3)\to \left(0,1\right)$的连续映射，$\left(0,1\right)$是闭集（没学过拓扑的人估计都不敢相信这句话），它的像是开集。