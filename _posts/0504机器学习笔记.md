---
title: 机器学习笔记
tags: 数学，计算机
categories: 学习笔记
date: 2019-05-04 17:42:17
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://wujilingfeng.top/MathJax/MathJax.js?config=TeX-AMS_CHTML">
</script>


本文记录机器学习笔记

<!--more-->

##### 信息熵

获取这里用”编码量“更”严谨“，假设一个事件的概率是$p_i$,则它的编码量是$-logp_i$,log以2为底。那么对于概率测度$\left(\Omega,A,\mu\right)$,其平均一次编码量为$H\left(p\right)=-E\left(log\mu\left(A\right)\right)$

###### 命题

$-E\left(log\mu\left(A\right)\right)=\sum p_ilog\frac{1}{p_i}\leq log\sum 1$等号成立的条件是$p_i$全相等

##### 相对熵

$KL\left(P,Q\right)=\sum p_ilog\frac{p_i}{q_i}$

易知$P=Q$时取最小值（证明方法同上）

有人用KL散度（编码量差）度量概率空间，这是不合理的。事实上概率空间可以用全变差来度量，和wassertein距离度量。

如果非要用到信息熵。我们可以对概率测度P做这样的变换$\frac{1}{H\left(p\right)}p_ilogp_i  $这是新的概率测度，我们可以衡量新生成的概率测度。

在向往的生活3中陈伟霆玩的游戏中我们了解到监督学习(博弈类学习)的困难。

#### 神经元（工程）

神经元适合模拟事物规律，所以神经元适合"学习"分类，识别问题。

神经元框架（编程）考虑物理原型（人的神经元细胞）是有好处的，这样当我们制造物理模型是可以兼容。

神经元的一个模块（函数）的输入理论上都可以化为向量形式，而当我们输入一串以时间作为标志的数据量时，所以我们可以允许输入变量是二维张量，他的默认计算方式也是显然的。

人类最根本的知识（技能）来自实践(相当于反编译器)

人类从长辈（书）学习的知识（技能）相当于编译器

神经元最重要的非线性控制系统(非线性迭代)没有添加
神经元的宽度决定学习能力，深度决定能力的稳定性
记忆，处理，输入，输出都可以看成一类，都可以用循环表示

#### 定义
逼近这里采用无穷泛数的定义
#### 命题 1

对于n个输入的函数f，(f是连续的，且$x_i$是紧集)则$\left(x_1, x_2,...x_n,f()\right)$是n维流形。那么对于任何的n维流形，一层神经网络就可以无限逼近这个流形（Relu函数）

*证明 :* 由[这里](https://wujilingfeng.top/2019/07/03/泛函分析笔记/fun_of_ana.pdf)的第四条,(欧式空间紧集上的函数以无穷泛数为泛数，他是紧的而且等价等度连续和一致有界)，我们可以在定义域$\Omega$空间找到$\epsilon$网格(在这些网格中函数差小于$\epsilon$）
然后我们可以用足够多的超平面划分这些网格。
接下来我们的手法如下：
{% asset_img 1.jpg picture1 %}
{% asset_img 2.jpg picture2 %}
{% asset_img 3.jpg picture3%}



#### 命题1（概率论）
对于n维空间$\Omega$上borel可测集，上有测度$f$。那么Relu函数也是上面的测度，且测度空间以无穷泛数作为泛数，Relu函数的线性组合可以逼近f.(用概率论的语言叙述命题1)

#### 命题2
有n层神经网络，每层(i层到i+1层)满足如下条件:
* $x \to Relu\left(W_i x+b_i\right)$,$W_i$是$m\times m$矩阵，且满秩
那么这个神经网络把凸集映为凸集


*证明：* 只需证明每层神经网络把凸集映为凸集
若$\Omega$属于支集则结论成立，若$\Omega$有不属于支集的部分，我们可以把非支集部分映到支集，保持值不变。
比如把$P_0$不属于支集，它的第一位数字超过支集，我们可以沿着$W_i$第2到第m的向量的公共法向量移动，让第一位数移到$W_i$第一个向量定义的超平面上，以下位数同理。

#### 推论
上述证明手法把定义域与支集相交，$Relu\left(W_ix+b\right)=W_ix+b$,又因为$W_i$满秩，所以可逆，故把非凸集映为非凸集
#### 推论
上面结论把Relu函数换成单调函数f也成立（比如softmax）。
由于仿射变环维持凸凹性，所以直接考察$y=f(x)$,$y_1,y_2\in f(x)$,那么$\frac{y_1+y_2}{2}\in f\left(x\right)$只需对每位数单独考虑即可

#### 推论
如上所述，若矩阵$W_i$满足行满秩，则结论成立
* 证明: * 如上

举例说明上述条件不可弱化
若行不满秩，假设 m=3(输入向量维数)
令
$$W_i=\begin{matrix}
1&0&0\\\\
0&1&0\\\\
1&1&0
\end{matrix}
$$
则$Relu\left(W_ix\right)$是非凸集
若假设m=2,则
$$W_i=\begin{matrix}
1&0\\\\
0&1\\\\
1&1
\end{matrix}
$$
则$Relu\left(W_ix\right)$是非凸集


#### 定义   （迭代序列）

有向量函数f，初始向量$x_0$,则$\left(x_0,x_1=f\left(x_0\right),...x_i=\left(x_{x-i}\right),....\right)$为迭代序列

###### 举例

$f\left(x\right)=1-\frac{x^2}{5}+x$,当$x_0\in [0,1]$迭代序列收敛，当$x_0\in [100,+\infty]$迭代序列发散

深度学习库推介使用pytorch